# Tooling Guide

Complete reference for tools, libraries, and when to use them.

## Core Language & Runtime

### Python 3.11+
- **Why:** Modern typing + Pydantic v2 support; wide library compatibility
- **Current:** Python 3.12.10 in this environment
- **Usage:** All code targets Python ≥3.11

## Schema → Code Pipeline (The Backbone)

### LinkML (CLI + Python libs)
- **Why:** Author schemas in YAML; generate Pydantic classes, SHACL, OWL
- **Install:** `linkml`, `linkml-runtime`
- **Key Commands:**
  ```bash
  linkml lint schemas/overlays/overlay.yaml
  linkml generate pydantic schemas/overlays/overlay.yaml > generated/pydantic/models.py
  ```
- **Provides:** `schemas/core.yaml` with mixins: `NodeProv`, `EdgeProv`, `entity_type`

### Instructor (Structured LLM Outputs)
- **Why:** Constrain LLM to emit `LinkMLSchemaSpec` JSON for linting and compilation
- **Install:** `instructor`
- **Usage:** Define response model, call with OpenAI/Anthropic client
- **Pattern:**
  ```python
  from instructor import from_openai
  client = from_openai(openai_client)
  spec = client.chat.completions.create(
      model="gpt-4o-mini",
      messages=[...],
      response_model=LinkMLSchemaSpec
  )
  ```

### Pydantic v2
- **Why:** Unified typed model layer (generated by LinkML + handwritten mixins)
- **Install:** `pydantic>=2.6`
- **Usage:** Validation at every stage; small handwritten core mixins

## Graph Layer (Storage, Retrieval, Ingestion)

### Graphiti (Neo4j-backed)
- **Why:** In-graph "episodes", hybrid search (semantic + BM25), Pydantic type registries
- **Usage:**
  - Register `ENTITY_TYPES`, `EDGE_TYPES`, `EDGE_TYPE_MAP`
  - `add_episode` for text/JSON
  - `add_triplet` for deterministic hierarchy
- **Files:**
  - `graphiti/registries.py` - Export registries from generated models
  - `graphiti/ingestion.py` - Helper functions
- **Fallback:** Mock adapter with same interface if Graphiti unavailable

### Neo4j (Database - Optional)
- **Why:** Vector + full-text indexes inside graph (HNSW + BM25); fits Graphiti
- **Install:** `neo4j>=5`
- **Setup:** Docker or local install
- **Note:** Can run fully mocked in v1

## Parsing & Chunking

### LlamaParse
- **Why:** PDF → structured text for standards & docs (AAOIFI, briefs, contracts)
- **Usage:** Produce paragraph/clause-level strings + page anchors for episodes
- **Install:** `llama-parse` (optional)

### LlamaIndex (Splitters Only - Optional)
- **Why:** Semantic/sentence-window splitters for nicer clauses
- **Install:** `llama-index-core` (minimal usage)
- **Note:** We don't use its indices/stores

## Evaluation & Observability

### Extractive Gate (Custom)
- **Why:** Zero unsupported tokens (quotes only + tiny connectives)
- **Implementation:** `agents/eval_gate.py`
- **Components:**
  - `AnswerCandidate` model
  - `GraphAccessor` protocol
  - `evaluate_candidate()` with coverage ≥0.98

### Langfuse (Optional in v1)
- **Why:** Traces/experiments for later iterations
- **Install:** `langfuse` (feature-flagged)
- **Usage:** Wire when credentials provided

### Other Eval Tools (Optional)
- **RAGAS / TruLens / Promptfoo / Giskard:** Additional eval and red-team tools
- **Status:** Stubs and TODOs; don't block on these

## Search, Crawl, References

### Exa API
- **Why:** Fetch canonical ontology snippets (DoCO/PROV/SKOS/FIBO) and examples
- **Usage:** `agents/ontology_retriever.py` → search class names/IRIs
- **Output:** `{name, iri, snippet, source_url}`
- **Fallback:** `--offline` flag with minimal fixtures

### Firecrawl
- **Why:** Crawl/scrape official pages resolved by Exa
- **Usage:** Same agent as Exa; cache text for prompts
- **Fallback:** Offline stubs if no keys

## CLI, Developer Experience, Testing

### Typer (CLI) + Rich (Output)
- **Why:** One-command demo runs; readable developer UX
- **Install:** `typer>=0.12`, `rich>=13`

### pytest
- **Why:** Codegen imports, ingestion stubs, eval-gate unit tests
- **Install:** `pytest>=8`

### Poetry or uv
- **Why:** Reproducible env; lockfile for CI
- **Choice:** Using pyproject.toml format

### Makefile
- **Targets:**
  - `make plan` - Print plan summary
  - `make codegen OVERLAY=...` - Lint + generate Pydantic
  - `make run-demo SPEC=...` - Full pipeline
  - `make test` - Run test suite
  - `make ingest-demo` - Mock ingestion

## Model Providers (LLM/Embeddings)

### LLM for Schema Synthesis
- **Options:** Anthropic (Claude) or OpenAI
- **Why:** Instructor supports both; provider-agnostic pipeline
- **Config:** Read `LLM_PROVIDER`, `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- **Current:** Defaulting to Anthropic (Claude Code environment)

### Embeddings/Rerankers
- **Status:** Not required for v1
- **When:** Add later if needed for advanced hybrid search

## Environment Variables (.env.example)

```bash
# LLM / Instructor
LLM_PROVIDER=anthropic|openai
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Ontology fetchers (optional)
EXA_API_KEY=
FIRECRAWL_API_KEY=

# Graph layer (optional for mock)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASS=pass

# Feature flags
USE_LLAMA_INDEX_SPLITTERS=false
USE_LANGFUSE=false
OFFLINE_MODE=true
```

## What Claude Code Should Do With Each Tool

### Planning & Scaffold
- Write `README.md`, repo tree, Makefile, `pyproject.toml`

### DSL Loader
- Validate `OutcomeSpec` YAML → object

### Ontology Retriever
- Call Exa/Firecrawl
- Cache IRIs + snippets to `artifacts/ontology_refs.json`
- Offline stub fallback

### Schema Synthesizer
- Run Instructor with constrained `LinkMLSchemaSpec`
- Output `overlay.yaml` (imports `schemas/core.yaml`)
- Also emit `eqp.json` (Evidence Query Plan)

### Codegen
- Run `linkml lint` + `linkml generate pydantic`
- Output to `generated/pydantic/models.py`
- Smoke-import test

### Graph Glue
- Build `ENTITY_TYPES`, `EDGE_TYPES`, `EDGE_TYPE_MAP` from generated models
- Ingestion helpers
- Deterministic doc→section→paragraph structure stamping
- Set `entity_type` on nodes and `uri` on edges

### Eval
- Implement extractive gate and tests
- Provide `answer-demo` that proposes quote spans, gates them, renders/refuses

### CI
- Script sequence: lint→codegen→tests→demo run
- Use GitHub MCP to open PRs (optional)

## Minimal Dependency List

```toml
[tool.poetry.dependencies]
python = "^3.11"
pydantic = "^2.6"
linkml = "^1.7"
linkml-runtime = "^1.7"
instructor = "^1.3"
typer = "^0.12"
rich = "^13"
pytest = "^8"
python-dotenv = "^1"
pydantic-settings = "^2"
httpx = "^0.27"
requests = "^2"
neo4j = "^5"
pyyaml = "^6"

# Optional
llama-index-core = {version = "^0.11", optional = true}
langfuse = {version = "^2", optional = true}
```

## Guardrails (Keep Loop Outcome-First)

1. **Keep overlays small** - ≤12 classes, ≤10 associations (fail build if exceeded)
2. **Always import `schemas/core.yaml`** - Guarantees `entity_type`, provenance, IDs
3. **Associations must be PascalCase** - If aligned to standards, stamp `slot_uri`
4. **First release answers are extractive only** - No paraphrase; gate at ≥0.98 quote coverage
5. **Provenance discipline** - Always set `entity_type` on nodes; set `uri` on edges when known

## Tool Decision Matrix

| Task | Tool | Fallback |
|------|------|----------|
| Schema definition | LinkML | Manual Pydantic |
| LLM output validation | Instructor | JSON schema validation |
| Ontology lookup | Exa → Firecrawl | Offline fixtures |
| Graph storage | Graphiti + Neo4j | Mock adapter |
| Evaluation | Custom extractive gate | N/A (core requirement) |
| CLI | Typer + Rich | Argparse |
| Testing | pytest | unittest |

## Performance Targets

- **P95 Time-to-Evidence:** <800ms (mock acceptable for v1)
- **Pipeline end-to-end:** <5 minutes on demo fixtures
- **Evaluation run:** <2 minutes for 10-20 question GoldenSet

## Next Steps After Tool Setup

1. Install all dependencies via poetry/pip
2. Create `.env` from `.env.example`
3. Test each tool in isolation (import tests)
4. Wire tools into pipeline stages
5. Run smoke tests on each component
6. Execute end-to-end demo
